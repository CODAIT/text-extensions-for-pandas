{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SpaCy dependency parser not found. Please run 'python -m spacy download en_core_web_trf', then restart your Jupyter kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-184aa7207128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_trf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-184aa7207128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_trf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     raise IOError(\"SpaCy dependency parser not found. Please run \"\n\u001b[0m\u001b[1;32m     30\u001b[0m                   \u001b[0;34m\"'python -m spacy download en_core_web_trf', then \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                   \"restart your Jupyter kernel.\")\n",
      "\u001b[0;31mOSError\u001b[0m: SpaCy dependency parser not found. Please run 'python -m spacy download en_core_web_trf', then restart your Jupyter kernel."
     ]
    }
   ],
   "source": [
    "# Import Python libraries\n",
    "from typing import *\n",
    "import os\n",
    "import ibm_watson\n",
    "import ibm_watson.natural_language_understanding_v1 as nlu\n",
    "import ibm_cloud_sdk_core\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "_PROJECT_ROOT = \"../..\"\n",
    "try:\n",
    "    import text_extensions_for_pandas as tp\n",
    "except ModuleNotFoundError as e:\n",
    "    # If we're running from within the project source tree and the parent Python\n",
    "    # environment doesn't have the text_extensions_for_pandas package, use the\n",
    "    # version in the local source tree.\n",
    "    if not os.getcwd().endswith(\"market\"):\n",
    "        raise e\n",
    "    if _PROJECT_ROOT not in sys.path:\n",
    "        sys.path.insert(0, _PROJECT_ROOT)\n",
    "    import text_extensions_for_pandas as tp\n",
    "    \n",
    "# Download the SpaCy model if necessary\n",
    "try:\n",
    "    spacy.load(\"en_core_web_trf\")\n",
    "except IOError:\n",
    "    raise IOError(\"SpaCy dependency parser not found. Please run \"\n",
    "                  \"'python -m spacy download en_core_web_trf', then \"\n",
    "                  \"restart JupyterLab.\")\n",
    "\n",
    "\n",
    "if \"IBM_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\"IBM_API_KEY environment variable not set. Please create \"\n",
    "                     \"a free instance of IBM Watson Natural Language Understanding \"\n",
    "                     \"(see https://www.ibm.com/cloud/watson-natural-language-understanding) \"\n",
    "                     \"and set the IBM_API_KEY environment variable to your instance's \"\n",
    "                     \"API key value.\")\n",
    "\n",
    "api_key = os.environ.get(\"IBM_API_KEY\")\n",
    "service_url = os.environ.get(\"IBM_SERVICE_URL\")  \n",
    "natural_language_understanding = ibm_watson.NaturalLanguageUnderstandingV1(\n",
    "    version=\"2021-01-01\",\n",
    "    authenticator=ibm_cloud_sdk_core.authenticators.IAMAuthenticator(api_key)\n",
    ")\n",
    "natural_language_understanding.set_service_url(service_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Use dependency parsing to extract executives' titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dependency parsing* is a natural language processing technique that identifies the relationships between the words that make up a sentence. We can treat these relationships between a sentence's words as the edges of a graph. This graph is always a tree, so we refer to it as the *dependency-based parse tree* of the sentence. \"Dependency-based parse tree\" is an awkward phrase, so it's common to refer to this tree as a \"dependency parse\" or a \"parse tree\".\n",
    "\n",
    "In this second part of the series, we'll use *dependency parsing* to break down these phrases into their component parts and extract out the\n",
    "\n",
    "\n",
    "\n",
    "*TODO: Diagram of a example dependency parse.*\n",
    "\n",
    "The detailed information in the parse tree allows us to quickly create a very general \n",
    "solution to many extraction tasks without needing to create complex rules or train a machine learning model. In this post, we'll use the dependency parsing to extract executives' titles from the phrases that our code from Part 1 produces.\n",
    "\n",
    "We'll use the dependency parser from the open source NLP library SpaCy. Text Extensions for Pandas includes a utility function that turns the output of SpaCy language models into a DataFrame. Here's what we get when we run the SpaCy language model over our example document and convert the output to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_language_model = spacy.load(\"en_core_web_trf\")\n",
    "all_token_features = tp.io.spacy.make_tokens_and_features(\"\"\"\n",
    "I like natural language processing.\n",
    "\"\"\", spacy_language_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "doc_text = step_2_results[\"subject\"].array.document_text\n",
    "\n",
    "spacy_language_model = spacy.load(\"en_core_web_trf\")\n",
    "all_token_features = tp.io.spacy.make_tokens_and_features(doc_text, spacy_language_model)\n",
    "all_token_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SpaCy language model output contains many different features for each token position in the document.\n",
    "We're only interested in the dependency parse, so let's project this language model output down to just the parts \n",
    "that are relevant to the parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_features = all_token_features[[\"id\", \"span\", \"tag\", \"dep\", \"head\"]]\n",
    "parse_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1, we used Text Extensions for Pandas and Watson Natural Language Understanding to identify locations where IBM press releases quoted a person by name. We walked through this process in great detail, but at high level, you can think of it as a two-step process:\n",
    "1. Use IBM Watson Natural Language Understanding to extract semantic roles and person mentions from the press release.\n",
    "2. Use Text Extensions for Pandas to convert those model outputs to Pandas DataFrames. Then cross-reference the data in those DataFrames to find the places where the press release quoted a person by name.\n",
    "\n",
    "*TODO: Describe how we've shared the code from Part 1 in `market_intelligence.py`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import market_intelligence as mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly recap what the output of those two processing steps looks. We'll use the same example document as in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc_url = \"https://newsroom.ibm.com/2021-01-04-IBM-Study-Majority-of-Surveyed-Companies-are-Not-Prepared-for-IT-Needs-of-the-Future-Say-U-S-and-U-K-Tech-Leaders\"\n",
    "example_doc_html = mi.download_article(example_doc_url)\n",
    "display(HTML(textwrap.shorten(example_doc_html, 5000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first processing step extracts named entities and semantic roles with IBM Watson Natural Language Understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1_results = (\n",
    "    mi.extract_named_entities_and_semantic_roles(example_doc_html, \n",
    "                                                 natural_language_understanding)\n",
    ")\n",
    "textwrap.shorten(str(step_1_results), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second processing step uses Text Extensions for Pandas to convert these model outputs into DataFrames, then uses these DataFrames to identify persons that the document quotes by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_2_results = mi.identify_persons_quoted_by_name(step_1_results)\n",
    "step_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noted at the end of Part 1, the phrase in the `subject` column of our DataFrame \n",
    "contains additional information about each executive's job position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_2_results.iloc[0][\"subject\"].covered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy's dependency parse is based on the [Universal Dependencies](https://universaldependencies.org/) framework. The parser gives each word, or *token*, in the document a part of speech (`tag` in the DataFrame above), a link to its *head* token (`head`), and a dependency type (`dep`).\n",
    "\n",
    "The parser's output covers all 826 tokens in the document. Let's filter down to just the tokens that overlap with the phrases we've previously identified as describing persons who made statements. We can use Text Extensions for Pandas' `contain_join()` span operation to implement this filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_tokens = (\n",
    "    tp.spanner.contain_join(step_2_results[\"subject\"], \n",
    "                            parse_features[\"span\"], \n",
    "                            \"subject\", \"span\")\n",
    "    .merge(parse_features)\n",
    "    .set_index(\"id\", drop=False)\n",
    ")\n",
    "phrase_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating the parse tree\n",
    "\n",
    "This subtree of the document's parse tree describes the relationships between the words in our target phrase. We can visualize these relationships by rendering the subtree with SpaCy's rendering engine, DisplaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.io.spacy.render_parse_tree(phrase_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start out with the parse tree nodes that comprise the person entity and traverse `appos` and `compound` links to build up likely titles.\n",
    "\n",
    "To facilitate this traversal, let's convert the graph in `phrase_tokens` into DataFrames of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = phrase_tokens[[\"id\", \"span\", \"tag\", \"subject\"]].reset_index(drop=True)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = phrase_tokens[[\"id\", \"head\", \"dep\"]].reset_index(drop=True)\n",
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the graph nodes that are parts of target person names. The span operation `overlap_join()` lets us efficiently correlate the spans in the Watson model output with the spans in the SpaCy model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_nodes = (\n",
    "    tp.spanner.overlap_join(step_2_results[\"person\"], nodes[\"span\"],\n",
    "                            \"person\", \"span\")\n",
    "    .merge(nodes)\n",
    ")\n",
    "person_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the set of edges that we will follow to expand this set of nodes. For this application, we will\n",
    "follow two types of dependency links: \n",
    "* [appositional modifier (`appos`)](https://universaldependencies.org/docs/en/dep/appos.html) links that connect\n",
    "  names to their associated titles; and\n",
    "* [compound](https://universaldependencies.org/docs/en/dep/compound.html) links that connect the components of\n",
    "  these titles to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_edges = edges[edges[\"dep\"].isin([\"appos\", \"compound\"])]\n",
    "filtered_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a set of starting nodes and a set of edges to traverse, so we can perform a [transitive closure](https://en.wikipedia.org/wiki/Transitive_closure) operation: \n",
    "Expand our set of nodes by traversing links of the graph; and keep doing so as long as there are additional nodes\n",
    "to be found.\n",
    "\n",
    "We can use the Pandas `merge` function to implement a single step of traversing links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all nodes that are on the other end of an edge from a node in `person_nodes`.\n",
    "selected_nodes = person_nodes.drop(columns=\"person\").copy()\n",
    "\n",
    "addl_nodes = (\n",
    "    selected_nodes[[\"id\"]]\n",
    "    .merge(filtered_edges, left_on=\"id\", right_on=\"head\", suffixes=[\"_head\", \"\"])[[\"id\"]]\n",
    "    .merge(nodes)\n",
    ")\n",
    "addl_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add these additional nodes to our set of ndoes and repeat the traversal step until\n",
    "the set stops growing, we have a transitive closure operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nodes = person_nodes.drop(columns=\"person\").copy()\n",
    "previous_num_nodes = 0\n",
    "\n",
    "# Keep going as long as the previous round added nodes to our set.\n",
    "while len(selected_nodes.index) > previous_num_nodes:\n",
    "    previous_num_nodes = len(selected_nodes.index)\n",
    "    \n",
    "    # Traverse one edge out from all nodes in `selected_nodes`\n",
    "    addl_nodes = (\n",
    "        selected_nodes[[\"id\"]]\n",
    "        .merge(filtered_edges, left_on=\"id\", right_on=\"head\", suffixes=[\"_head\", \"\"])[[\"id\"]]\n",
    "        .merge(nodes)\n",
    "    )\n",
    "    \n",
    "    # Add any previously unselected node to `selected_nodes`\n",
    "    selected_nodes = pd.concat([selected_nodes, addl_nodes]).drop_duplicates()\n",
    "\n",
    "selected_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the set of all nodes that are reachable from one of our selected person names by traversing\n",
    "`appos` and `compound` links in the dependency parse. If we filter out the nodes we started with, we\n",
    "should get the nodes for the tokens that comprise the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_nodes = selected_nodes[~selected_nodes[\"id\"].isin(person_nodes[\"id\"])]\n",
    "title_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to turn these sets of nodes into spans. We can Pandas' grouping\n",
    "and aggregation to do so, taking advantage of the fact that the \"addition\" operation \n",
    "for spans is defined as:\n",
    "```\n",
    "span1 + span2 = smallest span that contains both span1 and span2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = (\n",
    "    title_nodes\n",
    "    .groupby(\"subject\")\n",
    "    .aggregate({\"span\": \"sum\"})\n",
    "    .reset_index()\n",
    "    .rename(columns={\"span\": \"title\"})\n",
    ")\n",
    "# As of Pandas 1.5.1, groupby over extension types downgrades them to object dtype.\n",
    "# Cast back up to the extension type.\n",
    "titles_df[\"subject\"] = titles_df[\"subject\"].astype(tp.SpanDtype())\n",
    "\n",
    "titles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can join back with our DataFrame of person/company information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execs_with_titles_df = pd.merge(step_2_results, titles_df)\n",
    "execs_with_titles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have Python code that goes all the way from an HTML document to a DataFrame of names and titles of executives. We're ready to do some data mining! In the next part of this series, we'll apply the NLP code we've developed so far to many IBM press releases at once and extract the names and titles of many different executives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
