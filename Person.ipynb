{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pandas_text' from '/Users/freiss/pd/pandas_text/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependencies go in this cell.\n",
    "# The script env.sh in this directory should create an Anaconda environment with\n",
    "# all these dependencies installed\n",
    "\n",
    "# Python built-in packages\n",
    "import functools\n",
    "import importlib\n",
    "from typing import *\n",
    "\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# TEMPORARY until we can use Python 3.8 functools' built-in memoized property\n",
    "from memoized_property import memoized_property\n",
    "\n",
    "# [re]import our local library code\n",
    "import pandas_text as pt\n",
    "importlib.reload(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST_DOC_FILE = \"resources/example_doc.txt\"\n",
    "TEST_DOC_FILE = \"resources/short_example_doc.txt\"\n",
    "\n",
    "with open (TEST_DOC_FILE, \"r\") as f:\n",
    "    TEST_TEXT = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resources:\n",
    "    \"\"\"\n",
    "    Data structures that are loaded once, as opposed to recreated on\n",
    "    every document.\n",
    "    \n",
    "    This category includes tokenizers, dictionaries, and compiled regexes.\n",
    "    \n",
    "    Everything in this class is a cached property\n",
    "    \"\"\"\n",
    "    \n",
    "    @memoized_property\n",
    "    def Tokenizer(self):\n",
    "        nlp = spacy.lang.en.English()\n",
    "        return nlp.Defaults.create_tokenizer(nlp)\n",
    "    \n",
    "    \n",
    "    ############################\n",
    "    # DICTIONARIES\n",
    "    \n",
    "    @memoized_property\n",
    "    def GlobalFirstNameDict(self):\n",
    "        return pt.load_dict(\"resources/first_name.dict\", self.Tokenizer)\n",
    "    \n",
    "    @memoized_property\n",
    "    def GlobalLastNameDict(self):\n",
    "        return pt.load_dict(\"resources/last_name.dict\", self.Tokenizer)\n",
    "    \n",
    "    ############################\n",
    "    # REGEXES\n",
    "    \n",
    "    @memoized_property\n",
    "    def CapsWordRegex(self):\n",
    "        return regex.compile(\"[A-Z][a-z]*\")\n",
    "    \n",
    "\n",
    "class Document:\n",
    "    \"\"\"\n",
    "    Rules that define the fields of the current document.\n",
    "    \n",
    "    In this case the document is a single string.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_text: str, resources: Resources):\n",
    "        self._text = doc_text\n",
    "        self._resources = resources\n",
    "        \n",
    "    @property\n",
    "    def Text(self):\n",
    "        return self._text\n",
    "    \n",
    "    @memoized_property\n",
    "    def Tokens(self):\n",
    "        return pt.make_tokens(self.Text, self._resources.Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionaries:\n",
    "    \"\"\"\n",
    "    Rules that evaluate dictionaries against the document's raw tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: Document, resources: Resources):\n",
    "        self._d = d\n",
    "        self._resources = resources\n",
    "    \n",
    "    @memoized_property\n",
    "    def GlobalFirstName(self):\n",
    "        return pt.extract_dict(self._d.Tokens, self._resources.GlobalFirstNameDict)\n",
    "    \n",
    "    @memoized_property\n",
    "    def GlobalLastName(self):\n",
    "        return pt.extract_dict(self._d.Tokens, self._resources.GlobalLastNameDict)\n",
    "\n",
    "class Regexes:\n",
    "    \"\"\"\n",
    "    Rules that evaluate regular expressions against the document's raw tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: Document, resources: Resources):\n",
    "        self._d = d\n",
    "        self._resources = resources\n",
    "    \n",
    "    @property\n",
    "    def CapsWord(self):\n",
    "        \"\"\"\n",
    "        A single token that starts with a capital letter, with subsequent letters not\n",
    "        capitalized.\n",
    "        \"\"\"\n",
    "        return pt.extract_regex_tok(\n",
    "            token_offsets = self._d.Tokens[\"char_offsets\"],\n",
    "            target_str = self._d.Text,\n",
    "            compiled_regex = self._resources.CapsWordRegex)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonName:\n",
    "    \"\"\"\n",
    "    Rules that extract potential person name entities.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc: Document, dicts: Dictionaries, regexes: Regexes):\n",
    "        self._doc = doc\n",
    "        self._dicts = dicts\n",
    "        self._regexes = regexes\n",
    "    \n",
    "    @property\n",
    "    def Person1(self):\n",
    "        \"\"\"\n",
    "        <match of GlobalFirstName dict> <match of GlobalLastName dict>\n",
    "        \"\"\"\n",
    "        first = self._dicts.GlobalFirstName\n",
    "        last = self._dicts.GlobalLastName\n",
    "        return pt.adjacent_join(\n",
    "            first_series = first[\"matches\"],\n",
    "            second_series = last[\"matches\"],\n",
    "            first_name = \"first_name\",\n",
    "            second_name = \"last_name\",\n",
    "            min_gap = 0,\n",
    "            max_gap = 0)\n",
    "    \n",
    "    @property\n",
    "    def Person2(self):\n",
    "        \"\"\"\n",
    "        <match of GlobalFirstName dict> <capitalized word>\n",
    "        \"\"\"\n",
    "        first = self._dicts.GlobalFirstName\n",
    "        last = self._regexes.CapsWord\n",
    "        return pt.adjacent_join(\n",
    "            first_series = first[\"matches\"],\n",
    "            second_series = last[\"matches\"],\n",
    "            first_name = \"first_name\",\n",
    "            second_name = \"last_name\",\n",
    "            min_gap = 0,\n",
    "            max_gap = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = Resources()\n",
    "doc = Document(TEST_TEXT, resources)\n",
    "dicts = Dictionaries(doc, resources)\n",
    "regexes = Regexes(doc, resources)\n",
    "persons = PersonName(doc, dicts, regexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/freiss/opt/miniconda3/envs/pd/lib/python3.7/site-packages/pandas/core/indexing.py:2418: FutureWarning: DataFrame/Series.to_dense is deprecated and will be removed in a future version\n",
      "  result = result.to_dense()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[173, 174)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[283, 284)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[289, 290)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[299, 300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[279, 280)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[122, 123)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[269, 270)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[286, 287)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[214, 215)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[305, 306)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[306, 307)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[320, 321)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[275, 276)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       matches\n",
       "0   [173, 174)\n",
       "1   [283, 284)\n",
       "2   [289, 290)\n",
       "3   [299, 300)\n",
       "4   [279, 280)\n",
       "5       [4, 5)\n",
       "6   [122, 123)\n",
       "7   [269, 270)\n",
       "8      [9, 10)\n",
       "9   [286, 287)\n",
       "10  [214, 215)\n",
       "11  [305, 306)\n",
       "12  [306, 307)\n",
       "13  [320, 321)\n",
       "14  [275, 276)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts.GlobalFirstName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokseries = pd.Series(doc.Tokens)\n",
    "tokdf = pd.DataFrame({\n",
    "    \"token_id\": tokseries.index\n",
    "})\n",
    "tokdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokseries.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr(tokdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.Tokens[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regeTokens.CapsWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons.Person1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons.Person2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
